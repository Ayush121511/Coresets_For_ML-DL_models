{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxjLGXBAorz6"
      },
      "source": [
        "# Coreset for Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "kj0dfRWq_WM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "XsZK7YrCorz8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data PreProcessing"
      ],
      "metadata": {
        "id": "wQbpB5UrsCt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Extraction"
      ],
      "metadata": {
        "id": "WRcaHcNJs9mC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "F6NzKsiforz9"
      },
      "outputs": [],
      "source": [
        "data_init = pd.read_csv(\"bank-full.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ipOyZ6LGorz9"
      },
      "outputs": [],
      "source": [
        "arr_init = data_init.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "vZ7mMKPEorz9"
      },
      "outputs": [],
      "source": [
        "arr= []\n",
        "for i in range(arr_init.shape[0]):\n",
        "  cur = arr_init[i][0]\n",
        "  arr.append(cur.split(';'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(arr)):\n",
        "  for j in range(len(arr[0])):\n",
        "    try:\n",
        "      arr[i][j] = int(arr[i][j])\n",
        "    except:\n",
        "      arr[i][j] = str(arr[i][j])\n",
        "      arr[i][j] = arr[i][j].strip('\"')"
      ],
      "metadata": {
        "id": "0fLQPE71hsHv"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array(arr,dtype='object')"
      ],
      "metadata": {
        "id": "bVlAOa9eof9Z"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUaHTFdbotK0",
        "outputId": "0562e812-fde0-4af7-8526-ac965982517d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45211, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X =arr[:,:-1]\n",
        "Y = arr[:,-1]"
      ],
      "metadata": {
        "id": "ziO7bfglpKWP"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVESJ6F2zH2X",
        "outputId": "d453888f-01b7-4841-f02c-b96368338470"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58, 'management', 'married', 'tertiary', 'no', 2143, 'yes', 'no',\n",
              "       'unknown', 5, 'may', 261, 1, -1, 0, 'unknown'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.reshape(Y, (-1,1))"
      ],
      "metadata": {
        "id": "zkiaMHU4p7vm"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(Y.shape[0]):\n",
        "  if(Y[i]=='no'):\n",
        "    Y[i,0]=0\n",
        "  else:\n",
        "    Y[i,0] = 1"
      ],
      "metadata": {
        "id": "TlX8oL3aNif_"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihKFaf6lAz97",
        "outputId": "c9278b18-f264-4965-fbb4-a54135349600"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Enocding"
      ],
      "metadata": {
        "id": "CE8b1AKnsrhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OneHotEncoder(\n",
        "    categories='auto',  # Categories per feature\n",
        "    drop=None, # Whether to drop one of the features\n",
        "    sparse=True, # Will return sparse matrix if set True\n",
        "    # dtype= <class 'numpy.float64'>, # Desired data type of the output\n",
        "    handle_unknown='error' # Whether to raise an error\n",
        ")\n",
        "enc = OneHotEncoder()"
      ],
      "metadata": {
        "id": "A3yX2YLJyEpm"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(X.shape[1]):\n",
        "  if(type(X[0][j]) != type(1)):\n",
        "    X_j_new = enc.fit_transform(np.reshape(X[:,j],(-1,1)))\n",
        "    X_j_new = X_j_new.toarray()\n",
        "    X = np.concatenate((X,X_j_new),axis=1)\n"
      ],
      "metadata": {
        "id": "iAYlxPPWtBVu"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_aug = X\n",
        "p=0\n",
        "for j in range(X_aug.shape[1]):\n",
        "  if(type(X[0][j]) not in [type(1),type(1.0)] ):\n",
        "    X_aug = np.delete(X_aug,j-p,1)\n",
        "    p+=1\n",
        "X = X_aug"
      ],
      "metadata": {
        "id": "iyHaA-jn_ROr"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtT2ECkrA4nL",
        "outputId": "deac1476-1be5-4e7a-aacc-77c4ed3fd447"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58, 2143, 5, 261, 1, -1, 0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
              "       0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0,\n",
              "       0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
              "       0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Definitions"
      ],
      "metadata": {
        "id": "UP9tnoPmxReL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "LpKA1_uDor0A"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    z = z[0]\n",
        "    a =   np.exp(z)/(1+np.exp(z)) if z<0 else (1/(1+np.exp(-z)))\n",
        "    return a\n",
        "def sigmoid_der(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "PV8EgU78or0A"
      },
      "outputs": [],
      "source": [
        "def linear(z):\n",
        "    return z\n",
        "def linear_der(z):\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "TuRuymv3or0A"
      },
      "outputs": [],
      "source": [
        "def tanh(z):\n",
        "    return np.where( z<0, (np.exp(z) - 1/np.exp(z) ) / (np.exp(z) + 1/np.exp(z)) , ( 1/np.exp(-z) - np.exp(-z) ) / (np.exp(-z) + 1/np.exp(-z)) )\n",
        "def tanh_der(z):\n",
        "    return 1- tanh(z)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "UdVkVl23or0A"
      },
      "outputs": [],
      "source": [
        "def relu(z):\n",
        "    # print(z)\n",
        "    return z if z>0 else 0.1*z\n",
        "def relu_der(z):\n",
        "    z = z[0]\n",
        "    if(z==0):\n",
        "        z = z + 1e-10\n",
        "    return 1 if z>0 else 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "GLv1R-QBor0A"
      },
      "outputs": [],
      "source": [
        "def a(W,b,g,a_l_1):\n",
        "    z_l = np.matmul(W,a_l_1) + b\n",
        "    a_l=np.ones((z_l.shape[0],z_l.shape[1]))\n",
        "    for i in range(z_l.shape[0]):\n",
        "        a_l[i] = g(z_l[i])\n",
        "    return a_l,z_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Qpd0H92Kor0B"
      },
      "outputs": [],
      "source": [
        "def compute(X,W,B,G,N):\n",
        "    A=[]\n",
        "    Z=[]\n",
        "    a_l = z_l = X.T\n",
        "    A.append(a_l)\n",
        "    Z.append(z_l)\n",
        "    for l in range(1,len(N)):\n",
        "        a_l,z_l = a(W[l-1],B[l-1],G[l-1],a_l)\n",
        "        # print(a_l)\n",
        "        A.append(np.array(a_l))\n",
        "        Z.append(np.array(z_l))\n",
        "    return A,Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "vLgiU-Saor0B"
      },
      "outputs": [],
      "source": [
        "def binary_entropy_loss(truth_labels, predicted_labels):\n",
        "    epsilon = 1e-15\n",
        "    predicted_labels = np.clip(predicted_labels, epsilon, 1 - epsilon)\n",
        "    N = predicted_labels.shape[0]\n",
        "    truth_labels = np.reshape(truth_labels,(-1,1))\n",
        "    # print(truth_labels.shape,predicted_labels.shape)\n",
        "    loss = -np.sum(truth_labels * np.log(predicted_labels +1e-15) + (1 - truth_labels) * np.log(1 - predicted_labels+1e-15)) / N\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "kXc0O2Pbor0B"
      },
      "outputs": [],
      "source": [
        "def partial_derivative_binary_entropy_loss(truth_labels, predicted_labels):\n",
        "    N = predicted_labels.shape[0]\n",
        "    return (predicted_labels - 2*predicted_labels*truth_labels + truth_labels) / (predicted_labels * (1 - predicted_labels) * N +1e-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0jbs2sQor0B"
      },
      "source": [
        "## Coreset Construction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "bD9Baux8or0B"
      },
      "outputs": [],
      "source": [
        "def coresetConstr(X,Y,W_anc,B_anc,R,n,eps,delta,l,L):\n",
        "    G = [relu,relu,sigmoid]\n",
        "    G_der = [ relu_der,relu_der,sigmoid_der ]\n",
        "    Y_predicted=[]\n",
        "    M = - 1e+8\n",
        "\n",
        "    for i in range(0,X.shape[0]):\n",
        "        Der = 0\n",
        "        # forward propogation\n",
        "        A,Z = compute(np.array([X[i]]),W_anc,B_anc,G,n)\n",
        "        Y_predicted.append(A[3][0][0])\n",
        "        #backward propogation\n",
        "        dc_dz_last = np.multiply(partial_derivative_binary_entropy_loss(Y[i],A[3]) ,G_der[2](Z[3]))\n",
        "        der_3 = np.matmul(dc_dz_last,A[2].T)\n",
        "        Der += np.sum(der_3**2)\n",
        "        dc_dz_second= np.multiply(np.matmul(W_anc[2].T,dc_dz_last) , G_der[1](Z[2]))\n",
        "        der_2 = np.matmul(dc_dz_second,A[1].T)\n",
        "        Der+= np.sum(der_2**2)\n",
        "        dc_dz_first = np.multiply(np.matmul(W_anc[1].T,dc_dz_second) , G_der[0](Z[1]))\n",
        "        der_1 = np.matmul(dc_dz_first,A[0].T)\n",
        "        Der+= np.sum(der_1**2)\n",
        "        if(Der>M):\n",
        "            M=Der\n",
        "\n",
        "    Y = np.reshape( np.array(Y,dtype='float'), (Y.shape[0]) )\n",
        "    Y_predicted = np.reshape( np.array(Y_predicted,dtype='float'), (np.array(Y_predicted,dtype='float').shape[0]) )\n",
        "\n",
        "    H = log_loss(Y,Y_predicted)\n",
        "\n",
        "    N = int(np.log(X.shape[0]))\n",
        "    W = np.zeros((X.shape[0]))\n",
        "    M = M**(1/len(G))\n",
        "    P = []\n",
        "    for i in range(N-1):\n",
        "        P.append([])\n",
        "    for i in range(X.shape[0]):\n",
        "        loss =-1* (Y[i] * math.log2(Y_predicted[i] +1e-15) + (1 - Y[i]) * math.log2(1 - Y_predicted[i]+1e-15))\n",
        "        rel_loss = loss/H\n",
        "        if(rel_loss<=0):\n",
        "          rel_loss = -rel_loss + 1e-10\n",
        "        if(math.log2(rel_loss)>0):\n",
        "          P[int(math.log2(rel_loss))].append(i)\n",
        "        else:\n",
        "          P[0].append(i)\n",
        "\n",
        "    for i in range(len(P)):\n",
        "        Q_i_s = (H * 2**(i-1) + M*R +L*R**2)**2*delta**(-2) * np.log(1/l)\n",
        "        if(Q_i_s<len(P[i])):\n",
        "            Q_i = np.random.choice(P[i],int(Q_i_s))\n",
        "        else:\n",
        "            Q_i = P[i]\n",
        "\n",
        "        for i in Q_i:\n",
        "            W[i] = int(Q_i_s)/len(P)\n",
        "\n",
        "    return W\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "hPqrpfu6or0B"
      },
      "outputs": [],
      "source": [
        "N = [X.shape[1],5,5,1]\n",
        "G = [relu,relu,sigmoid]\n",
        "W = []\n",
        "B = []\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\n",
        "for l in range(1,len(N)):\n",
        "    W_l = np.random.rand(N[l],N[l-1])/10\n",
        "    B_l = np.zeros((N[l],1))\n",
        "    # print(W_l.shape,B_l.shape)\n",
        "    W.append(W_l)\n",
        "    B.append(B_l)\n",
        "Weights = coresetConstr(X_train,Y_train,W,B,1,N,0.5,1,0.8,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "ATcyH7tKor0B"
      },
      "outputs": [],
      "source": [
        "size = 0\n",
        "for i in range(X_train.shape[0]):\n",
        "    if(Weights[i]!=0):\n",
        "        size+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "vheI91Ulor0B",
        "outputId": "e32f6164-f0a1-423e-ab4c-e13a269d8fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1899"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ],
      "source": [
        "size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train).astype('float32')\n",
        "Y_train = np.asarray(Y_train).astype('float32')\n",
        "Weights = np.asarray(Weights).astype('float32')\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train,Weights))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)"
      ],
      "metadata": {
        "id": "DvSrQGTuJsEu"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTaXXtUXOsiZ",
        "outputId": "9754d005-483c-45ae-f327-5d79c9e96ff7"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 51), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(10, activation = 'relu',   name = \"L1\"),\n",
        "        Dense(5, activation = 'relu', name = \"L2\"),\n",
        "        Dense(1,activation = 'sigmoid', name='L3')\n",
        "    ]\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")"
      ],
      "metadata": {
        "id": "_iOonIG8LR8Z"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train, epochs=10)"
      ],
      "metadata": {
        "id": "_tQKtbR8Oblc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e6f623-0978-4bf3-b51d-2ce312b9ee6b"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1131/1131 [==============================] - 2s 2ms/step - loss: 0.3348\n",
            "Epoch 2/10\n",
            "1131/1131 [==============================] - 2s 2ms/step - loss: 0.3036\n",
            "Epoch 3/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2983\n",
            "Epoch 4/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2896\n",
            "Epoch 5/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2847\n",
            "Epoch 6/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2727\n",
            "Epoch 7/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2635\n",
            "Epoch 8/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2631\n",
            "Epoch 9/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2548\n",
            "Epoch 10/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2537\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e4961f621d0>"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_weights()[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnBvEUQN1BAu",
        "outputId": "b67d432d-3059-488e-ca68-38ec77173fe8"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.asarray(X_test).astype('float32')\n",
        "Y_test = np.asarray(Y_test).astype('float32')\n",
        "Y_predicted = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dE7uNQsOl1x",
        "outputId": "8990f299-6db5-4f59-d7c7-e71159b4ca81"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "283/283 [==============================] - 0s 805us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_predicted"
      ],
      "metadata": {
        "id": "EPV9xWVchGlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3c8951-1a1e-4840-9249-dc15544b3daa"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.27522287],\n",
              "       [0.09540436],\n",
              "       [0.00782357],\n",
              "       ...,\n",
              "       [0.03653059],\n",
              "       [0.01503961],\n",
              "       [0.0034468 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test = Y_test.reshape(-1,1)"
      ],
      "metadata": {
        "id": "uRew-THgRxii"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_loss(Y_test,Y_predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3umkvho0ytQ",
        "outputId": "dd1c2f16-704d-4550-a258-ac6ec9a9d99e"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2664012062153163"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(Y_predicted.shape[0]):\n",
        "  if(Y_predicted[i,0]>=0.5):\n",
        "    Y_predicted[i,0]=1\n",
        "  else:\n",
        "    Y_predicted[i,0]=0"
      ],
      "metadata": {
        "id": "PMGYlFjKO5s2"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.where((Y_test[:,0] == Y_predicted[:,0]),1,0))/Y_test.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgU65a-9SQUv",
        "outputId": "8177fde0-e429-4e9f-de43-baae0029d1f9"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8737144752847507"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Coresets"
      ],
      "metadata": {
        "id": "rYgMgYvBRYi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def difference(W,B,Weights):\n",
        "  diff =0\n",
        "  n = 0\n",
        "  for i in range(len(W)):\n",
        "    diff+= np.sum((W[i].T - Weights[2*i])**2)\n",
        "    diff+= np.sum((np.reshape(B[i],(-1,)) - Weights[2*i+1])**2)\n",
        "    n+= W[i].shape[0]*W[i].shape[1] + B[i].shape[0]\n",
        "  return diff**(1/2)\n"
      ],
      "metadata": {
        "id": "MiksFanBCMlm"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seqCoreSets(X,Y,R,eps,delta,l,L):\n",
        "  N = [X.shape[1],5,5,1]\n",
        "  G = [relu,relu,sigmoid]\n",
        "  W = []\n",
        "  B = []\n",
        "  Weight_layers = []\n",
        "  for l in range(1,len(N)):\n",
        "    W_l = np.random.rand(N[l],N[l-1])/1000\n",
        "    B_l = np.zeros((N[l],1))\n",
        "    W_l_i = [W_l.T,B_l[:,0]]\n",
        "    # print(W_l.shape,B_l.shape)\n",
        "    W.append(W_l)\n",
        "    B.append(B_l)\n",
        "    Weight_layers.append(W_l_i)\n",
        "  W_cur = W\n",
        "  B_cur = B\n",
        "  X = np.asarray(X).astype('float32')\n",
        "  Y = np.asarray(Y).astype('float32')\n",
        "  tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "  model = Sequential(\n",
        "      [\n",
        "          Dense(5, activation = 'relu', weights = Weight_layers[0], name = \"L1\"),\n",
        "          Dense(5, activation = 'relu',  weights = Weight_layers[1],name = \"L2\"),\n",
        "          Dense(1,activation = 'sigmoid', weights = Weight_layers[2], name='L3')\n",
        "      ]\n",
        "  )\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      optimizer=tf.keras.optimizers.Adam(0.0005),\n",
        "  )\n",
        "  model.build(input_shape = X.shape)\n",
        "  model.set_weights([Weight_layers[0][0],Weight_layers[0][1],Weight_layers[1][0],Weight_layers[1][1],Weight_layers[2][0],Weight_layers[2][1]])\n",
        "  Weights_cur = model.get_weights()\n",
        "  H= 100\n",
        "  first=True\n",
        "  s_id = 0\n",
        "  while(H>0.3 and s_id<10):\n",
        "    s_id+=1\n",
        "    print(\"set id:\",s_id)\n",
        "\n",
        "    Weights = coresetConstr(X,Y,W,B,1,N,0.5,1,0.8,100)\n",
        "\n",
        "    size = 0\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        if(Weights[i]!=0):\n",
        "            size+=1\n",
        "    print(\"size:\",size)\n",
        "\n",
        "    Weights = np.asarray(Weights).astype('float32')\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X,Y,Weights))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "    diff=R-1\n",
        "    epochs = 0\n",
        "    while(diff<R and H>0.3 and epochs<10):\n",
        "      model.build()\n",
        "      model.set_weights(Weights_cur)\n",
        "\n",
        "      epochs+=1\n",
        "      print(\"epoch:\",epochs)\n",
        "\n",
        "      history = model.fit(train_dataset,epochs=1)\n",
        "\n",
        "      Weights_cur = model.get_weights()\n",
        "      diff = difference(W,B,Weights_cur)\n",
        "      print(\"diff:\",diff)\n",
        "\n",
        "    Y_predicted = model.predict(X)\n",
        "    Y_predicted = np.reshape(Y_predicted,(Y_predicted.shape[0]))\n",
        "    Y = np.reshape(Y,(Y.shape[0]))\n",
        "    H = log_loss(Y,Y_predicted)\n",
        "    Weights_cur = model.get_weights()\n",
        "    for i in range(len(W)):\n",
        "\n",
        "      W[i] = Weights_cur[2*i].T\n",
        "      B[i] = np.reshape(Weights_cur[2*i+1],(-1,1))\n",
        "\n",
        "    print(\"H:\",H)\n",
        "\n",
        "  return Weights"
      ],
      "metadata": {
        "id": "WiOYfQghSjAh"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weights= seqCoreSets(X_train,Y_train,0.5,0.5,1,0.8,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6du9gG-0P0E",
        "outputId": "39de18d7-5dc5-4363-b1bf-d5501b5930c3"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set id: 1\n",
            "size: 2192\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 9.8799\n",
            "diff: 0.23301066636794227\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 8.7745\n",
            "diff: 0.4419070421986361\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 7.9134\n",
            "diff: 0.6353816308265584\n",
            "1131/1131 [==============================] - 1s 812us/step\n",
            "H: 0.4986983898723994\n",
            "set id: 2\n",
            "size: 3955\n",
            "epoch: 1\n",
            "566/566 [==============================] - 2s 1ms/step - loss: 13.0917\n",
            "diff: 0.271633093737475\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 11.9487\n",
            "diff: 0.4867529794281466\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 11.2295\n",
            "diff: 0.6699559728547362\n",
            "1131/1131 [==============================] - 1s 811us/step\n",
            "H: 0.3908584030406151\n",
            "set id: 3\n",
            "size: 3981\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.9006\n",
            "diff: 0.15973830350888246\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.6863\n",
            "diff: 0.29361498448848417\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.4716\n",
            "diff: 0.4080082186217088\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4239\n",
            "diff: 0.4954625416886715\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3532\n",
            "diff: 0.5639307581521705\n",
            "1131/1131 [==============================] - 1s 831us/step\n",
            "H: 0.3595878331313354\n",
            "set id: 4\n",
            "size: 3955\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3489\n",
            "diff: 0.03570496974348725\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.3317\n",
            "diff: 0.065017222125416\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.2673\n",
            "diff: 0.08846247394551142\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.2526\n",
            "diff: 0.10270202040212499\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3661\n",
            "diff: 0.10390722679453467\n",
            "epoch: 6\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.2565\n",
            "diff: 0.11433565715785401\n",
            "epoch: 7\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.2704\n",
            "diff: 0.11883580580522185\n",
            "epoch: 8\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3469\n",
            "diff: 0.11375415272389794\n",
            "epoch: 9\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3366\n",
            "diff: 0.11063063198370839\n",
            "epoch: 10\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3388\n",
            "diff: 0.11051010878500832\n",
            "1131/1131 [==============================] - 1s 826us/step\n",
            "H: 0.35827945902535147\n",
            "set id: 5\n",
            "size: 3970\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3586\n",
            "diff: 0.0003827810287475586\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4296\n",
            "diff: 0.0036724805502997256\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3666\n",
            "diff: 0.003074407568270035\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3283\n",
            "diff: 0.003227829878133962\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3618\n",
            "diff: 0.0020601749282208267\n",
            "epoch: 6\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4217\n",
            "diff: 0.0010064840246175938\n",
            "epoch: 7\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3173\n",
            "diff: 0.004109501809290673\n",
            "epoch: 8\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.3842\n",
            "diff: 0.0011430978712865018\n",
            "epoch: 9\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.4219\n",
            "diff: 0.0025143622899900997\n",
            "epoch: 10\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4465\n",
            "diff: 0.010584354475821955\n",
            "1131/1131 [==============================] - 1s 815us/step\n",
            "H: 0.35834664017007417\n",
            "set id: 6\n",
            "size: 3970\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.2775\n",
            "diff: 0.01177740072913486\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3313\n",
            "diff: 0.014769673323900116\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3091\n",
            "diff: 0.015137910795957542\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.3940\n",
            "diff: 0.01210260367751389\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.3377\n",
            "diff: 0.012491345459617276\n",
            "epoch: 6\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3545\n",
            "diff: 0.01350796205801599\n",
            "epoch: 7\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4078\n",
            "diff: 0.00912201403247349\n",
            "epoch: 8\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3477\n",
            "diff: 0.010163187946394304\n",
            "epoch: 9\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3591\n",
            "diff: 0.012076258536391401\n",
            "epoch: 10\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4086\n",
            "diff: 0.00845944877995456\n",
            "1131/1131 [==============================] - 1s 825us/step\n",
            "H: 0.3582919949157144\n",
            "set id: 7\n",
            "size: 4005\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4411\n",
            "diff: 0.002655029296875\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4472\n",
            "diff: 0.004382729619502307\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4471\n",
            "diff: 0.00479137897343159\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4234\n",
            "diff: 0.006371378986722218\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4359\n",
            "diff: 0.005369424899345042\n",
            "epoch: 6\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4669\n",
            "diff: 0.002268433592787956\n",
            "epoch: 7\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4929\n",
            "diff: 0.00024068355560302734\n",
            "epoch: 8\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.5386\n",
            "diff: 0.004482627002158716\n",
            "epoch: 9\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.4777\n",
            "diff: 0.008744239755123544\n",
            "epoch: 10\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.4487\n",
            "diff: 0.001743316650390625\n",
            "1131/1131 [==============================] - 1s 821us/step\n",
            "H: 0.3583026349716069\n",
            "set id: 8\n",
            "size: 3983\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4321\n",
            "diff: 0.0015699863252806319\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4278\n",
            "diff: 0.001108527177122924\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4155\n",
            "diff: 0.0012643337024959911\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.3412\n",
            "diff: 0.006930947228934266\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3821\n",
            "diff: 0.007550001230991857\n",
            "epoch: 6\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4220\n",
            "diff: 0.004571676347526142\n",
            "epoch: 7\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3881\n",
            "diff: 0.005815386810032562\n",
            "epoch: 8\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3497\n",
            "diff: 0.009105920888384229\n",
            "epoch: 9\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4135\n",
            "diff: 0.006666660428209183\n",
            "epoch: 10\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4153\n",
            "diff: 0.003498077392578125\n",
            "1131/1131 [==============================] - 1s 857us/step\n",
            "H: 0.3582816354299745\n",
            "set id: 9\n",
            "size: 4008\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4591\n",
            "diff: 0.0021581649780273438\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4217\n",
            "diff: 0.004390597445399497\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4661\n",
            "diff: 0.0016912222202664288\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4560\n",
            "diff: 0.002861380554738454\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4911\n",
            "diff: 0.0012205839098891147\n",
            "epoch: 6\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4489\n",
            "diff: 0.0020996331864007586\n",
            "epoch: 7\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4785\n",
            "diff: 0.0006833076477050781\n",
            "epoch: 8\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4528\n",
            "diff: 0.0026797056171604418\n",
            "epoch: 9\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4473\n",
            "diff: 0.003991007717633005\n",
            "epoch: 10\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.4340\n",
            "diff: 0.005085587556015486\n",
            "1131/1131 [==============================] - 1s 866us/step\n",
            "H: 0.35825341590505827\n",
            "set id: 10\n",
            "size: 3983\n",
            "epoch: 1\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4753\n",
            "diff: 0.007242083454329876\n",
            "epoch: 2\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3884\n",
            "diff: 0.0031604766845703125\n",
            "epoch: 3\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4222\n",
            "diff: 0.0049252509839976324\n",
            "epoch: 4\n",
            "566/566 [==============================] - 1s 2ms/step - loss: 10.3937\n",
            "diff: 0.005643725353655771\n",
            "epoch: 5\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4289\n",
            "diff: 0.0057353973388671875\n",
            "epoch: 6\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3636\n",
            "diff: 0.0022919177885024787\n",
            "epoch: 7\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3490\n",
            "diff: 0.00011289119720458984\n",
            "epoch: 8\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3931\n",
            "diff: 0.00213038921022674\n",
            "epoch: 9\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.3690\n",
            "diff: 0.0012574195861816406\n",
            "epoch: 10\n",
            "566/566 [==============================] - 1s 1ms/step - loss: 10.4214\n",
            "diff: 0.003830075262121884\n",
            "1131/1131 [==============================] - 1s 837us/step\n",
            "H: 0.3582744226224033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Weights"
      ],
      "metadata": {
        "id": "WUBFYfAI9lm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf8aab8-6098-4da6-ccde-2d58ee3630f2"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 0\n",
        "for i in range(X_train.shape[0]):\n",
        "    if(Weights[i]!=0):\n",
        "        print(Weights[i],end=' ')\n",
        "        size+=1"
      ],
      "metadata": {
        "id": "TyxB9yQN7LqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67be4db4-c70c-47a2-b90f-f6089a4e9831"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 259.33334 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 253.0 253.0 253.0 259.33334 259.33334 253.0 253.0 253.0 259.33334 253.0 259.33334 259.33334 253.0 259.33334 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 253.0 259.33334 259.33334 259.33334 253.0 253.0 259.33334 259.33334 253.0 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 259.33334 259.33334 259.33334 259.33334 259.33334 253.0 253.0 253.0 253.0 259.33334 253.0 253.0 259.33334 253.0 259.33334 253.0 253.0 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size"
      ],
      "metadata": {
        "id": "hBFe9oFYOZ-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e949544f-bfea-421b-ae31-46e5fd65ada6"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3983"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[0]"
      ],
      "metadata": {
        "id": "W1S6wKeeOftH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca0ee67-88e2-461f-b029-45371212331f"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36168"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Output"
      ],
      "metadata": {
        "id": "fDQmF-4ZQrVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train).astype('float32')\n",
        "Y_train = np.asarray(Y_train).astype('float32')\n",
        "Weights = np.asarray(Weights).astype('float32')\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train,Weights))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)"
      ],
      "metadata": {
        "id": "1ZaM8f3NRkW_"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "ob6yTvEbRkXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8719429-62d4-4768-9ce3-183e467945e6"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 51), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(10, activation = 'relu',   name = \"L1\"),\n",
        "        Dense(5, activation = 'relu', name = \"L2\"),\n",
        "        Dense(1,activation = 'sigmoid', name='L3')\n",
        "    ]\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")"
      ],
      "metadata": {
        "id": "aAFIgv3gRkXA"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train, epochs=10)"
      ],
      "metadata": {
        "id": "qpblQ-uwRkXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40be831f-633c-4fc5-cce7-58e43c066b0c"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1131/1131 [==============================] - 2s 1ms/step - loss: 2.0780\n",
            "Epoch 2/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.3757\n",
            "Epoch 3/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.3520\n",
            "Epoch 4/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.3435\n",
            "Epoch 5/10\n",
            "1131/1131 [==============================] - 2s 2ms/step - loss: 0.3176\n",
            "Epoch 6/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2960\n",
            "Epoch 7/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2780\n",
            "Epoch 8/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2608\n",
            "Epoch 9/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2581\n",
            "Epoch 10/10\n",
            "1131/1131 [==============================] - 1s 1ms/step - loss: 0.2522\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e494b2e0c10>"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_weights()[1]"
      ],
      "metadata": {
        "id": "vicW4mnuRkXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53a155a-4359-47b7-ce1f-259b992b715b"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.53797835, -0.10412534, -0.06277367, -0.04400063,  0.32237226,\n",
              "       -0.26154304, -0.2775164 ,  0.37491566,  0.25920147, -0.09035591],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.asarray(X_test).astype('float32')\n",
        "Y_test = np.asarray(Y_test).astype('float32')\n",
        "Y_predicted = model.predict(X_test)"
      ],
      "metadata": {
        "id": "QtPYklGHRkXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d2e1d8-2912-4d89-cd26-f3d3dfa8984b"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "283/283 [==============================] - 0s 910us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_predicted"
      ],
      "metadata": {
        "id": "t1Rpc3pgRkXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2dbd46-bd14-4358-db6c-18be40f0cf6e"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3036783 ],\n",
              "       [0.03031088],\n",
              "       [0.00664023],\n",
              "       ...,\n",
              "       [0.00657426],\n",
              "       [0.00840195],\n",
              "       [0.00403276]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test = Y_test.reshape(-1,1)"
      ],
      "metadata": {
        "id": "SvNdyJG9RkXB"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(Y_predicted.shape[0]):\n",
        "  if(Y_predicted[i,0]>=0.5):\n",
        "    Y_predicted[i,0]=1\n",
        "  else:\n",
        "    Y_predicted[i,0]=0"
      ],
      "metadata": {
        "id": "tMcjJo_0RkXB"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.where((Y_test[:,0] == Y_predicted[:,0]),1,0))/Y_test.shape[0]"
      ],
      "metadata": {
        "id": "N0WkrGX_RkXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11674413-f72e-4806-d4ea-fcfd95d561e5"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8886431493973239"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IVaE95B3EeF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CE8b1AKnsrhT",
        "UP9tnoPmxReL",
        "w0jbs2sQor0B",
        "rYgMgYvBRYi3"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}